proxmox:
  # -- Proxmox VE credentials
  secret:
    # -- The name of an existing credentials secret for Proxmox VE.
    name: proxmox-secret
    # -- The namespace of the existing credentials secret for Proxmox VE. When null or empty the release namespace will be used.
    namespace: ""

ipamProvider:
  # -- Enable the IPAMProvider usage
  enabled: true
  # -- IPAMProvider ranges
  ranges:
    - "192.168.100.10-192.168.100.250"
  # -- IPAMProvider prefix
  prefix: "24"
  # -- IPAMProvider gateway
  gateway: "192.168.100.1"

cluster:
  # -- Cluster name. If unset, the release name will be used
  name: ""
  # -- Define cluster with Proxmox cluster template resources or not
  useProxmoxClusterTemplateCR: false
  clusterNetwork:
    # -- API Server port
    apiServerPort: 6443
    # -- Service Domain for cluster DNS
    serviceDomain: cluster.local
    pods:
      # -- CIDR range for pods
      cidrBlocks: ["10.93.0.0/16"]
    services: 
      # -- CIDR range for services
      cidrBlocks: ["10.96.0.0/16"]
  applyResources:
    - name: flannel
      kind: ConfigMap
  controlPlane:
    # -- Labels to add to the control plane
    labels:
      cni: flannel
    # -- Number of control plane replicas
    replicas: 2
    # -- KamajiControlPlane dataStoreName
    dataStoreName: default
    # Set to null or remove the key entirely to disable the addon
    # addons:
    #   coreDNS: {}
    #   kubeProxy: {}
    #   konnectivity: null
    # -- Configure addons for the control plane
    addons:
      # -- CoreDNS addon configuration
      coreDNS: {}
      # -- Kube-proxy addon configuration
      kubeProxy: {}
      # -- Konnectivity addon configuration
      konnectivity: {}
    # -- Configure how KamajiControlPlane deployment should be done
    deployment:
      # -- Additional metadata as labels and annotations
      additionalMetadata: 
        labels: {}
        annotations: {}
      # -- Pods Additional metadata as labels and annotations
      podAdditionalMetadata:
        labels: {}
        annotations: {}
      # -- Affinity scheduling rules
      affinity: {}
      # -- Tolerations for scheduling
      tolerations: []
      # -- NodeSelector for scheduling
      nodeSelector:
        kubernetes.io/os: linux
      # -- TopologySpreadConstraints for scheduling
      topologySpreadConstraints: []
    kubelet:
      # -- kubelet cgroupfs configuration
      cgroupfs: systemd
      # -- kubelet preferredAddressTypes order
      preferredAddressTypes:
        - InternalIP
        - ExternalIP
        - Hostname
    network:
      # -- Ingress configuration (optional)
      # ingress: {}
      # -- Type of service used to expose the Kubernetes API server
      serviceType: LoadBalancer
      # -- Address used to expose the Kubernetes API server. If not set, the service will be exposed on the first available address.
      # serviceAddress: "10.10.10.120"
      # When using MetalLB to expose the Control Plane, set as the following to specify a specific IP address for the service
      # serviceAnnotations:
      #   metallb.io/loadBalancerIPs: "10.10.10.120"
      # -- Labels to use for the control plane service
      serviceLabels: {}
      # -- List of additional Subject Alternative Names to use for the API Server serving certificate
      certSANs: []
    # -- Kubernetes version
    version: v1.31.4
  # -- Proxmox VE nodes used for VM deployments
  allowedNodes:
  - labs
  credentialsRef:
    # -- Proxmox VE credentials secret name
    name: sample-proxmox-secret
    # -- Proxmox VE credentials secret namespace
    namespace: default
  # -- List of nameservers used by the machines
  dnsServers:
    - 8.8.8.8
  # -- IPv4Config contains information about address pool
  ipv4Config:
    # -- Proxmox VE machines address pool
    addresses: 
    - 192.168.100.10
    # -- Proxmox VE machines gateway
    gateway: 192.168.100.1
    # -- Proxmox VE machines prefix
    prefix: 24
  metrics:
    # -- Enable metrics collection. ServiceMonitor custom resource definition must be installed on the Management cluster.
    enabled: false
    # -- ServiceAccount for scraping metrics
    serviceAccount:
      # -- ServiceAccount name used for scraping metrics
      name: kube-prometheus-stack-prometheus
      # -- ServiceAccount namespace
      namespace: monitoring-system

nodePools:
  - name: default
    # -- Number of worker VMs instances
    replicas: 1
    # -- Enable machine health check resources (doesn't work)
    machineHealthCheck:
      enabled: false
    autoscaling:
      # -- Enable autoscaling
      enabled: false
      labels:
        # -- Labels to use for autoscaling: make sure to use the same labels on the autoscaler configuration
        autoscaling: "enabled"
      # -- Minimum number of instances in the pool
      minSize: "1"
      # -- Maximum number of instances in the pool
      maxSize: "6"
    # -- Proxmox VE node that hosts the VM template to be used to provision VMs
    sourceNode: pve
    # -- Proxmox VE nodes used for VM deployments
    allowedNodes:
      - labs
    # -- Proxmox VE resource pool to use
    pool: ""
    # -- Proxmox VE VM ID range for nodes
    vmIDRange:
      start: 200
      end: 210
    # -- Proxmox VE template ID to clone (alternatively, use TemplateSelector)
    templateId: 100
    # -- Proxmox VE template selector (alternatively, use templateId)
    templateSelector:
      # -- Proxmox VE template match tags
      matchTags:
      - "ubuntu-nobe-kube-1.31.4"
    # Proxmox VE skip check guest agent install
    skipQemuGuestAgent: false
    # -- Proxmox VE default network for VMs
    network:
      # -- Proxmox VE network bridge to use
      bridge: vmbr0
      # -- Proxmox VE network interface model to use
      model: virtio
      # -- Proxmox VE network interface dns servers. Overrides the setting in ProxmoxCluster
      dnsServers:
      - "8.8.8.8"
      # -- Use an IPAMProvider pool to reserve IPs
      addressesFromPools:
        # -- Enable the IPAMProvider usage
        enabled: true
    # -- Proxmox VE storage name for full clone
    storage: local
    # -- Proxmox VE disk format
    format: qcow2
    # -- Proxmox VE disk configuration
    disks:
      # -- Proxmox VE disk for the boot volume
      bootVolume:
        # -- Proxmox VE disk bus type
        disk: scsi0
        # -- Proxmox VE disk size in GB. The disk size must be greater than the template disk size
        sizeGb: 25
    # -- Memory to allocate to worker VMs
    memoryMiB: 8192
    # -- Number of cores to allocate to worker VMs
    numCores: 2
    # -- Number of sockets to allocate to worker VMs
    numSockets: 2
    # -- Additional cloud-init files to pass to the machines
    additionalCloudInitFiles: |
      #cloud-config
      timezone: Europe/Rome
    # -- users to create on machines
    users:
    - name: ubuntu
      # -- SSH shell to use
      shell: /bin/bash
      # -- SSH password to use. Use mkpasswd -m sha-512 to generate the password
      # passwd: "$6$E0UW ..."
      # -- Lock login password
      lockPassword: true
      # -- SSH public key to add
      sshAuthorizedKeys: []
      # -- sudoers configuration
      sudo: ALL=(ALL) NOPASSWD:ALL
    
    # -- Commands to run before kubeadm command
    preKubeadmCommands: []
    # -- Commands to run after kubeadm command
    postKubeadmCommands: []
    # -- Define cloud-config or ignition (flatcar) format
    cloudInitFormat: cloud-config
    # -- Configuration for ignition (flatcar)
    ignitionConfig: ""
    # -- Configuration for MachineHealthCheck unhealthy node timeout
    unhealthyNodeConditionsTimeout: 300
    # -- Configuration for MachineHealthCheck node startup timeout 
    nodeStartupTimeout: 900
    # -- Labels to add to the node pool when joining the cluster
    #labels: "node.kubernetes.io/node=labs"
    # -- Taints to add to the node pool when joining the cluster
    #taints: "node.kubernetes.io/node=labs:NoSchedule"
  
customRessources:
  - name: flannel
    kind: ConfigMap
    data: |
      ---
      kind: Namespace
      apiVersion: v1
      metadata:
        name: kube-flannel
        labels:
          k8s-app: flannel
          pod-security.kubernetes.io/enforce: privileged
      ---
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        labels:
          k8s-app: flannel
        name: flannel
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        verbs:
        - get
      - apiGroups:
        - ""
        resources:
        - nodes
        verbs:
        - get
        - list
        - watch
      - apiGroups:
        - ""
        resources:
        - nodes/status
        verbs:
        - patch
      ---
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        labels:
          k8s-app: flannel
        name: flannel
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: flannel
      subjects:
      - kind: ServiceAccount
        name: flannel
        namespace: kube-flannel
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        labels:
          k8s-app: flannel
        name: flannel
        namespace: kube-flannel
      ---
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: kube-flannel-cfg
        namespace: kube-flannel
        labels:
          tier: node
          k8s-app: flannel
          app: flannel
      data:
        cni-conf.json: |
          {
            "name": "cbr0",
            "cniVersion": "0.3.1",
            "plugins": [
              {
                "type": "flannel",
                "delegate": {
                  "hairpinMode": true,
                  "isDefaultGateway": true
                }
              },
              {
                "type": "portmap",
                "capabilities": {
                  "portMappings": true
                }
              }
            ]
          }
        net-conf.json: |
          {
            "Network": "10.244.0.0/16",
            "EnableNFTables": false,
            "Backend": {
              "Type": "vxlan"
            }
          }
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: kube-flannel-ds
        namespace: kube-flannel
        labels:
          tier: node
          app: flannel
          k8s-app: flannel
      spec:
        selector:
          matchLabels:
            app: flannel
        template:
          metadata:
            labels:
              tier: node
              app: flannel
          spec:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/os
                      operator: In
                      values:
                      - linux
            hostNetwork: true
            priorityClassName: system-node-critical
            tolerations:
            - operator: Exists
              effect: NoSchedule
            serviceAccountName: flannel
            initContainers:
            - name: install-cni-plugin
              image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
              command:
              - cp
              args:
              - -f
              - /flannel
              - /opt/cni/bin/flannel
              volumeMounts:
              - name: cni-plugin
                mountPath: /opt/cni/bin
            - name: install-cni
              image: ghcr.io/flannel-io/flannel:v0.27.4
              command:
              - cp
              args:
              - -f
              - /etc/kube-flannel/cni-conf.json
              - /etc/cni/net.d/10-flannel.conflist
              volumeMounts:
              - name: cni
                mountPath: /etc/cni/net.d
              - name: flannel-cfg
                mountPath: /etc/kube-flannel/
            containers:
            - name: kube-flannel
              image: ghcr.io/flannel-io/flannel:v0.27.4
              command:
              - /opt/bin/flanneld
              args:
              - --ip-masq
              - --kube-subnet-mgr
              resources:
                requests:
                  cpu: "100m"
                  memory: "50Mi"
              securityContext:
                privileged: false
                capabilities:
                  add: ["NET_ADMIN", "NET_RAW"]
              env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: EVENT_QUEUE_DEPTH
                value: "5000"
              - name: CONT_WHEN_CACHE_NOT_READY
                value: "false"
              volumeMounts:
              - name: run
                mountPath: /run/flannel
              - name: flannel-cfg
                mountPath: /etc/kube-flannel/
              - name: xtables-lock
                mountPath: /run/xtables.lock
            volumes:
            - name: run
              hostPath:
                path: /run/flannel
            - name: cni-plugin
              hostPath:
                path: /opt/cni/bin
            - name: cni
              hostPath:
                path: /etc/cni/net.d
            - name: flannel-cfg
              configMap:
                name: kube-flannel-cfg
            - name: xtables-lock
              hostPath:
                path: /run/xtables.lock
                type: FileOrCreate

